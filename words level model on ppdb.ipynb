{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import itertools\n",
    "import nltk\n",
    "import numpy as np\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from numpy import array\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras import objectives\n",
    "from keras.layers import Input, RepeatVector, LSTM, Embedding\n",
    "from keras.models import Model\n",
    "from keras.backend import permute_dimensions, gather\n",
    "from numpy import array\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import TensorBoard\n",
    "from time import time\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 100\n",
    "\n",
    "original_data = []\n",
    "paraphrase_data = []\n",
    "\n",
    "with open('source.txt', 'r', encoding = 'utf8') as f:\n",
    "    for line in f:\n",
    "        original_data.append(line.strip())\n",
    "\n",
    "\n",
    "with open('target.txt', 'r', encoding = 'utf8') as f:\n",
    "    for line in f:\n",
    "        paraphrase_data.append(line.strip())\n",
    "        \n",
    "paraphrase_data = paraphrase_data[:length]\n",
    "original_data = original_data[:length]\n",
    "print (len(original_data))\n",
    "print (len(paraphrase_data))\n",
    "print (original_data)\n",
    "print (paraphrase_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = []\n",
    "for word in original_data:\n",
    "    if word not in vocab:\n",
    "        vocab.append(word)\n",
    "\n",
    "for word in paraphrase_data:\n",
    "    if word not in vocab:\n",
    "        vocab.append(word)\n",
    "        \n",
    "# print (len(vocab))  \n",
    "# print (vocab)\n",
    "vocab_index = {}\n",
    "count = 1\n",
    "for word in vocab:\n",
    "    vocab_index[word] = count\n",
    "    count += 1\n",
    "    \n",
    "vocab_index['EOS'] = 0   \n",
    "# print (vocab_index)\n",
    "vocab_size = len(vocab_index)  \n",
    "print (vocab_size)\n",
    "\n",
    "int_to_vocab = {}\n",
    "for character, value in vocab_index.items():\n",
    "    int_to_vocab[value] = character\n",
    "\n",
    "# print (int_to_vocab)\n",
    "\n",
    "orig_data = []\n",
    "for word in original_data:\n",
    "    orig_data.append([vocab_index[word],vocab_index['EOS']])\n",
    "    \n",
    "para_data = []\n",
    "for word in paraphrase_data:\n",
    "    para_data.append([vocab_index[word],vocab_index['EOS']])    \n",
    "\n",
    "    \n",
    "def manual_one_hot(lines):\n",
    "    res = []\n",
    "    for line in lines:\n",
    "        res.append(to_categorical(line, num_classes=vocab_size))\n",
    "    return array(res)    \n",
    "# print (orig_data)\n",
    "# print (para_data)    \n",
    "\n",
    "para_data = np.array(para_data)  \n",
    "orig_data = np.array(orig_data)\n",
    "\n",
    "target_paraphrase = manual_one_hot(para_data)\n",
    "\n",
    "\n",
    "print (orig_data[0])   \n",
    "print (para_data[0])\n",
    "# print (len(orig_data[0]))\n",
    "# print (orig_data)\n",
    "print (orig_data.shape)\n",
    "# print (len(para_data[0]))\n",
    "print (para_data.shape)\n",
    "print (target_paraphrase.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = dict()\n",
    "f = open('glove.6B.50d.txt', encoding = 'utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (vocab_size)\n",
    "embedding_matrix = np.zeros((vocab_size, 50))\n",
    "for word, i in vocab_index.items():\n",
    "#     print (word, i)\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "#     print (embedding_vector, word, i)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        print (word, i)\n",
    "        \n",
    "print (embedding_matrix.shape)   \n",
    "print (embeddings_index['dealt'] - embedding_matrix[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.nan)\n",
    "input_dim = len(vocab_index)\n",
    "time_steps = orig_data.shape[1]\n",
    "batch_size = 1\n",
    "print(\"time_steps:\" + str(time_steps))\n",
    "print(\"input_dim:\" + str(input_dim))\n",
    "interdim = 128\n",
    "latentdim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_vae(input_dim,\n",
    "                    timesteps,\n",
    "                    batch_size,\n",
    "                    intermediate_dim,\n",
    "                    latent_dim,\n",
    "                    epsilon_std=1., ):\n",
    "    \"\"\"\n",
    "    Creates an LSTM Variational Autoencoder (VAE). Returns VAE, Encoder, Generator. \n",
    "\n",
    "    # Arguments\n",
    "        input_dim: int.\n",
    "        timesteps: int, input timestep dimension.\n",
    "        batch_size: int.\n",
    "        intermediate_dim: int, output shape of LSTM. \n",
    "        latent_dim: int, latent z-layer shape. \n",
    "        epsilon_std: float, z-layer sigma.\n",
    "\n",
    "\n",
    "    # References\n",
    "        - [Building Autoencoders in Keras](https://blog.keras.io/building-autoencoders-in-keras.html)\n",
    "        - [Generating sentences from a continuous space](https://arxiv.org/abs/1511.06349)\n",
    "    \"\"\"\n",
    "\n",
    "    def crop_dimension():\n",
    "        def func(x):\n",
    "            x = K.permute_dimensions(x, (1, 0, 2))\n",
    "            x = K.gather(x, [i for i in range(timesteps)])\n",
    "            x = K.permute_dimensions(x, (1, 0, 2))\n",
    "            return x\n",
    "\n",
    "        return Lambda(func)\n",
    "\n",
    "    def crop(dimension, start, end):\n",
    "        # Crops (or slices) a Tensor on a given dimension from start to end\n",
    "        # example : to crop tensor x[:, :, 5:10]\n",
    "        # call slice(2, 5, 10) as you want to crop on the second dimension\n",
    "        def func(x):\n",
    "            if dimension == 0:\n",
    "                return x[start: end]\n",
    "            if dimension == 1:\n",
    "                return x[:, start: end]\n",
    "            if dimension == 2:\n",
    "                return x[:, :, start: end]\n",
    "            if dimension == 3:\n",
    "                return x[:, :, :, start: end]\n",
    "            if dimension == 4:\n",
    "                return x[:, :, :, :, start: end]\n",
    "\n",
    "        return Lambda(func)\n",
    "\n",
    "    crop_layer = crop_dimension()\n",
    "    # drop_out_layer = Dropout(rate=0.0)\n",
    "    \n",
    "                                    # original sentence encoding\n",
    "        \n",
    "    original_sentence_input = Input(shape=(None,), name=\"OriginalInput_1\")\n",
    "    # original_sentence_input = Input(shape=(timesteps, input_dim,), name=\"OriginalInput_1\")\n",
    "    \n",
    "    original_encoder_layer_1 = LSTM(intermediate_dim, return_sequences=True, name=\"OriginalEncoderLSTM_1\")\n",
    "    original_encoder_layer_2 = LSTM(intermediate_dim, return_sequences=True, name=\"OriginalEncoderLSTM_2\")\n",
    "    original_encoder_layer_3 = LSTM(intermediate_dim, return_sequences=True, name=\"OriginalEncoderLSTM_3\")\n",
    "\n",
    "    embedding_layer = Embedding(198,50, weights = [embedding_matrix], mask_zero=True, trainable = False)\n",
    "\n",
    "    encoded_original = embedding_layer(original_sentence_input)\n",
    "    encoded_original = original_encoder_layer_1(encoded_original)\n",
    "    # encoded_original = drop_out_layer(encoded_original)\n",
    "    encoded_original = original_encoder_layer_2(encoded_original)\n",
    "    # encoded_original = drop_out_layer(encoded_original)\n",
    "    encoded_original = original_encoder_layer_3(encoded_original)\n",
    "\n",
    "    # paraphrase sentence encoder\n",
    "    paraphrase_sentence_encoder_layer1 = LSTM(intermediate_dim, return_sequences=True, name=\"ParaphraseEncoderLSTM_1\")\n",
    "    paraphrase_sentence_encoder_layer2 = LSTM(intermediate_dim, return_sequences=True, name=\"ParaphraseEncoderLSTM_2\")\n",
    "    paraphrase_sentence_encoder_layer3 = LSTM(intermediate_dim, return_sequences=True, name=\"ParaphraseEncoderLSTM_3\")\n",
    "\n",
    "    # x = Input(shape=(timesteps, input_dim,), name=\"ParaphraseInput_1\")\n",
    "    x = Input(shape=(None,), name=\"ParaphraseInput_1\")\n",
    "    embedded_x = embedding_layer(x)\n",
    "    # LSTM encoding\n",
    "    # h = LSTM(intermediate_dim)(x)\n",
    "    \n",
    "                                    # merge original and paraphrase\n",
    "        \n",
    "    encoded_original = crop_layer(encoded_original)\n",
    "    embedded_x = crop_layer(embedded_x)\n",
    "    \n",
    "    h = keras.layers.concatenate([encoded_original, embedded_x], axis=-1)\n",
    "    h = paraphrase_sentence_encoder_layer1(h)\n",
    "    # h = drop_out_layer(h)\n",
    "    h = paraphrase_sentence_encoder_layer2(h)\n",
    "    # h = drop_out_layer(h)\n",
    "    h = paraphrase_sentence_encoder_layer3(h)\n",
    "\n",
    "    # VAE Z layer\n",
    "    # h = Dense(intermediate_dim)(h)\n",
    "    # h = LSTM(intermediate_dim, return_sequences=True)(h)\n",
    "    # h = RepeatVector(timesteps)(h)\n",
    "    # h = LSTM(intermediate_dim)(h)\n",
    "\n",
    "    # test begin\n",
    "    # z_mean = Dense(latent_dim)(h)\n",
    "    # z_log_sigma = Dense(latent_dim)(h)\n",
    "    # test ends\n",
    "    z_mean = LSTM(latent_dim)(h)\n",
    "    z_log_sigma = LSTM(latent_dim)(h)\n",
    "\n",
    "    def sampling(args):\n",
    "        z_mean, z_log_sigma = args\n",
    "        epsilon = K.random_normal(shape=(batch_size, latent_dim),\n",
    "                                  mean=0., stddev=epsilon_std)\n",
    "        # return z_mean + z_log_sigma * epsilon\n",
    "        return z_mean + K.exp(z_log_sigma) * epsilon\n",
    "        # return z_mean + K.exp(z_log_sigma / 2) * epsilon\n",
    "\n",
    "    # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "    # so you could write `Lambda(sampling)([z_mean, z_log_sigma])`\n",
    "    z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_sigma])\n",
    "\n",
    "                                    # decoded LSTM layer\n",
    "        \n",
    "    # decoder_h = LSTM(timesteps, return_sequences=True, name=\"DecoderH_1\")\n",
    "    # decoder_mean = LSTM(input_dim, return_sequences=True, name=\"DecoderMean_1\")\n",
    "    decoder_h = Dense(intermediate_dim, name=\"DecoderH_1\", activation=\"relu\")\n",
    "    decoder_mean = Dense(input_dim, name=\"DecoderMean_1\", activation=\"sigmoid\")\n",
    "\n",
    "    # paraphrase sentence decoder\n",
    "    paraphrase_sentence_decoder_layer1 = LSTM(intermediate_dim, return_sequences=True,\n",
    "                                              name=\"ParaphraseSentenceDecoder_1\")\n",
    "    paraphrase_sentence_decoder_layer2 = LSTM(intermediate_dim, return_sequences=True,\n",
    "                                              name=\"ParaphraseSentenceDecoder_2\")\n",
    "    paraphrase_sentence_decoder_layer3 = LSTM(intermediate_dim, return_sequences=True,\n",
    "                                              name=\"ParaphraseSentenceDecoder_3\")\n",
    "\n",
    "    h_decoded = RepeatVector(timesteps)(z)\n",
    "\n",
    "    paraphrase_sentence_decoded = paraphrase_sentence_decoder_layer1(encoded_original)\n",
    "\n",
    "    Lambda(lambda x: K.permute_dimensions(x, (1, 0, 2)))\n",
    "    paraphrase_sentence_decoded = keras.layers.concatenate([h_decoded, paraphrase_sentence_decoded],\n",
    "                                                           axis=-1)\n",
    "    paraphrase_sentence_decoded = paraphrase_sentence_decoder_layer2(paraphrase_sentence_decoded)\n",
    "    paraphrase_sentence_decoded = keras.layers.concatenate([h_decoded, paraphrase_sentence_decoded],\n",
    "                                                           axis=-1)\n",
    "    paraphrase_sentence_decoded = paraphrase_sentence_decoder_layer3(paraphrase_sentence_decoded)\n",
    "\n",
    "    h_decoded = decoder_h(paraphrase_sentence_decoded)\n",
    "\n",
    "                                    # decoded layer\n",
    "    x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "                                # end-to-end autoencoder\n",
    "    vae = Model([original_sentence_input, x], x_decoded_mean)\n",
    "\n",
    "                            # encoder, from inputs to latent space\n",
    "    encoder = Model([original_sentence_input, x], z_mean)\n",
    "\n",
    "    # generator, from latent space to reconstructed inputs\n",
    "    decoder_latent_input = Input(shape=(latent_dim,), name=\"DecoderLatentInput_1\")\n",
    "    decoder_input_repeated = RepeatVector(timesteps)(decoder_latent_input)\n",
    "    # decoder original encoder\n",
    "    # decoder_original_input = Input(shape=(timesteps, input_dim,), name=\"DecoderOriginalInput_1\")\n",
    "    decoder_original_input = Input(shape=(None,), name=\"DecoderOriginalInput_1\")\n",
    "    decoder_original_encoded = embedding_layer(decoder_original_input)\n",
    "    decoder_original_encoded = original_encoder_layer_1(decoder_original_encoded)\n",
    "    # decoder_original_encoded = drop_out_layer(decoder_original_encoded)\n",
    "    decoder_original_encoded = original_encoder_layer_2(decoder_original_encoded)\n",
    "    # decoder_original_encoded = drop_out_layer(decoder_original_encoded)\n",
    "    decoder_original_encoded = original_encoder_layer_3(decoder_original_encoded)\n",
    "\n",
    "    paraphrase_sentence_decoded = paraphrase_sentence_decoder_layer1(decoder_original_encoded)\n",
    "    paraphrase_sentence_decoded = crop_layer(paraphrase_sentence_decoded)\n",
    "    paraphrase_sentence_decoded = keras.layers.concatenate([decoder_input_repeated, paraphrase_sentence_decoded],\n",
    "                                                           axis=-1)\n",
    "    # paraphrase_sentence_decoded = drop_out_layer(paraphrase_sentence_decoded)\n",
    "    paraphrase_sentence_decoded = paraphrase_sentence_decoder_layer2(paraphrase_sentence_decoded)\n",
    "    paraphrase_sentence_decoded = keras.layers.concatenate([decoder_input_repeated, paraphrase_sentence_decoded],\n",
    "                                                           axis=-1)\n",
    "    # paraphrase_sentence_decoded = drop_out_layer(paraphrase_sentence_decoded)\n",
    "    paraphrase_sentence_decoded = paraphrase_sentence_decoder_layer3(paraphrase_sentence_decoded)\n",
    "\n",
    "    _h_decoded = decoder_h(paraphrase_sentence_decoded)\n",
    "    _x_decoded_mean = decoder_mean(_h_decoded)\n",
    "    generator = Model([decoder_latent_input, decoder_original_input], _x_decoded_mean)\n",
    "\n",
    "    # def vae_loss(x, x_decoded_mean):\n",
    "    #     xent_loss = objectives.mse(x, x_decoded_mean)\n",
    "    #     kl_loss = - 0.5 * K.mean(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma))\n",
    "    #     loss = xent_loss + kl_loss\n",
    "    #     return loss\n",
    "    def vae_loss(x, x_decoded_mean):\n",
    "        xent_loss = objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "        kl_loss = - 0.5 * K.mean(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma), axis=-1)\n",
    "        return xent_loss + kl_loss\n",
    "\n",
    "    # def vae_loss(x, x_decoded_mean):\n",
    "    #     xent_loss = input_dim * metrics.binary_crossentropy(x, x_decoded_mean)\n",
    "    #     kl_loss = - 0.5 * K.sum(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma), axis=-1)\n",
    "    #     return K.mean(xent_loss + kl_loss)\n",
    "    # def vae_loss(x, x_decoded_mean):\n",
    "    #     # E[log P(X|z)]\n",
    "    #     recon = K.sum(K.binary_crossentropy(x, x_decoded_mean), axis=1)\n",
    "    #     # D_KL(Q(z|X) || P(z|X)); calculate in closed form as both dist. are Gaussian\n",
    "    #     kl = 0.5 * K.sum(K.exp(z_log_sigma) + K.square(z_mean) - 1. - z_log_sigma, axis=1)\n",
    "    #     return recon + kl\n",
    "\n",
    "    # sgd = SGD(lr=0.00005, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    #sgd = SGD(lr=0.00005)\n",
    "    # rmsprop = RMSprop()\n",
    "    vae.compile(optimizer=Adam(lr = 0.00001), loss=vae_loss, metrics = ['accuracy'])\n",
    "\n",
    "    return vae, encoder, generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae, enc, gen = create_lstm_vae(input_dim, timesteps=time_steps, batch_size=batch_size,\n",
    "                                intermediate_dim=interdim, latent_dim=latentdim, epsilon_std=1.,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.load_weights(\"vae_model.h5\")\n",
    "enc.load_weights(\"enc.h5\")\n",
    "gen.load_weights(\"gen.h5\") \n",
    "\n",
    "history = vae.fit([orig_data, para_data], target_paraphrase, epochs=10, batch_size=batch_size, shuffle=True, validation_split=0.2,\n",
    "#                     validation_data=([test_original, test_paraphrase], test_paraphrase),\n",
    "#                     callbacks=[TensorBoard(log_dir=\"logs_ppdb/{}\".format(time()))]\n",
    "                 )\n",
    " \n",
    "vae.save_weights(\"vae_model.h5\")\n",
    "enc.save_weights(\"enc.h5\")\n",
    "gen.save_weights(\"gen.h5\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(10):\n",
    "    t = random.randint(0,99)\n",
    "    sent_encoded = enc.predict([orig_data[t:t + 1], para_data[t:t + 1]], batch_size=batch_size)  \n",
    "    sent_decoded = gen.predict([sent_encoded, orig_data[t:t + 1]], batch_size=batch_size)\n",
    "    print ('prediction', end = ' ')\n",
    "    for n in sent_decoded[0]:\n",
    "        if int_to_vocab[np.argmax(n)] != '<PAD>' and int_to_vocab[np.argmax(n)] != 'EOS': \n",
    "            print(int_to_vocab[np.argmax(n)], end=\" \")    \n",
    "    print ('\\noriginal ', original_data[t])\n",
    "    print ('paraphrase ', paraphrase_data[t])\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(history.history.keys())\n",
    "# # summarize history for accuracy\n",
    "# plt.plot(history.history['acc'])\n",
    "# plt.plot(history.history['val_acc'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()\n",
    "# # summarize history for loss\n",
    "# # print (history.history['loss'])\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
